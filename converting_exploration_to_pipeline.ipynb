{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2ee82c-e800-47e2-8978-bc66d227da73",
   "metadata": {},
   "source": [
    "### //TODO:\n",
    "Add component for testing the model after deployment\n",
    "\n",
    "Add component for model monitoring\n",
    "\n",
    "Split the preprocess into BQ component and transform component\n",
    "\n",
    "Add a tensorflow model\n",
    "\n",
    "Add a pytorch model\n",
    "\n",
    "Add AutoML model\n",
    "\n",
    "Add Error handling\n",
    "\n",
    "Add mondel monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b04cb1f-d96c-4720-a1d8-b2460de481e2",
   "metadata": {},
   "source": [
    "# Create & Deploy Vertex-AI Pipeline w/ Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140d27f-f516-49a9-9ff4-811fd263feeb",
   "metadata": {},
   "source": [
    "Install the needed libraries in order to run the code locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9560fe6f-8a20-476a-aea3-f4c4abaf37dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install google-cloud-aiplatform==1.0.0 --upgrade\n",
    "!pip3 install kfp google-cloud-pipeline-components==0.1.1 --upgrade\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install google-cloud-aiplatform --upgrade\n",
    "!pip3 install pandas\n",
    "!pip3 install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617816bc-8237-4c94-b00f-6d62530516fe",
   "metadata": {},
   "source": [
    "Might need to restart kernel after initial installation of the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a0f438-f584-457e-b469-90d5ec9e9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from kfp import dsl\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, component, Metrics)\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5a343-6bcb-45e6-8227-eca403590f91",
   "metadata": {},
   "source": [
    "Getting some preset environment variables save to a local file. Create one of your own by following these instructions: https://stackoverflow.com/a/54028874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3affb3e-fcbf-4167-abe7-04b619d69cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "kubeflow-demos\n",
      "user-group-demo\n",
      "gs://user-group-demo/pipeline_root\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/a/54028874\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "BUCKET_NAME = os.environ['BUCKET']\n",
    "\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root'.format(BUCKET_NAME)\n",
    "REGION = 'us-central1'\n",
    "\n",
    "print(PROJECT_ID)\n",
    "print(BUCKET_NAME)\n",
    "print(PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d89156-3085-47e4-9eec-85c7c53f96d7",
   "metadata": {},
   "source": [
    "## 1. Create a component for reading data from BQ into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2298853a-ae42-42f3-8594-4b906552a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"google-cloud-aiplatform\", \"google-cloud-bigquery-storage\",\"google-cloud-bigquery\",\"pyarrow\"], output_component_file=\"preprocess.yaml\")\n",
    "def preprocess(output_csv_path: OutputPath('CSV')):\n",
    "    #1\n",
    "    from google.cloud import bigquery\n",
    "    import google.auth\n",
    "    \n",
    "    creds, project = google.auth.default()\n",
    "    client = bigquery.Client(project='kubeflow-demos', credentials=creds)\n",
    "\n",
    "    query =     \"\"\"\n",
    "            SELECT * FROM `kubeflow-demos.telco.churn`\n",
    "    \"\"\"\n",
    "    print(query)\n",
    "    \n",
    "    dataframe = client.query(query).to_dataframe()\n",
    "    print(dataframe.head())\n",
    "    \n",
    "    dataframe.to_csv(output_csv_path)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fdda2e-1254-4502-9f4f-9fdca5863c7a",
   "metadata": {},
   "source": [
    "## 2. Create a component to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "54350599-3b5a-4e4c-8b01-25030ef103e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"imbalanced-learn\", \"google-cloud-aiplatform\", \"pyarrow\"])\n",
    "def train(input_csv_path: InputPath('CSV'), saved_model: Output[Model], artifact_uri: OutputPath(str), accuracy: Output[Metrics], model_type: str, project_id: str, bucket: str):\n",
    "    from google.cloud import aiplatform\n",
    "    from typing import NamedTuple\n",
    "    #Train\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    print(len(df))\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype=='object':    #Since we are encoding object datatype to integer/float\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(df[c].values))\n",
    "            df[c] = lbl.transform(df[c].values)\n",
    "    print(df.head())  #To check if properly encoded\n",
    "    \n",
    "    X = df[['Contract', 'tenure', 'TechSupport', 'OnlineSecurity', 'TotalCharges', 'PaperlessBilling',\n",
    "       'DeviceProtection', 'Dependents', 'OnlineBackup', 'SeniorCitizen', 'MonthlyCharges',\n",
    "       'PaymentMethod', 'Partner', 'PhoneService']] #taking only relevant columns\n",
    "    y = df['Churn']\n",
    "\n",
    "\n",
    "    # Scaling all the variables to a range of 0 to 1\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    features = X.columns.values\n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    scaler.fit(X)\n",
    "    X = pd.DataFrame(scaler.transform(X))\n",
    "    X.columns = features\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "    #Choose which model to train\n",
    "    if model_type == 'linear_regression':\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        model = LogisticRegression()\n",
    "        \n",
    "    elif model_type == 'naive_bayes':\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        model = GaussianNB()\n",
    "        \n",
    "    elif model_type == 'decision_tree':\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        model = DecisionTreeClassifier()\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #Save the model to disk and also automatically to GCS\n",
    "    import joblib\n",
    "    \n",
    "    joblib.dump(model, os.path.join(saved_model.path.replace(\"saved_model\",\"\"), 'model.joblib'))\n",
    "    print(\" saved_model.path: \"+ saved_model.path)\n",
    "    print(\" saved_model.uri: \"+ saved_model.uri)\n",
    "    with open(artifact_uri, 'w') as f:\n",
    "        f.write(saved_model.uri.replace(\"saved_model\",\"\"))\n",
    "    \n",
    "    print(saved_model.uri)\n",
    "    \n",
    "    accuracy.log_metric('accuracy', 71.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfccc2d-7633-4579-9c88-f9b6ef368172",
   "metadata": {},
   "source": [
    "## 3. Eval component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d1ebe330-78f6-4de7-aaea-c989cde694ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def eval(baseline: float, accuracy: Input[Metrics], accuracy2: Input[Metrics], accuracy3: Input[Metrics]) -> bool:\n",
    "    isBetter = False\n",
    "    \n",
    "    print(str(dir(accuracy)))\n",
    "    new_val = float(accuracy.metadata['accuracy'])\n",
    "    print(str(new_val))\n",
    "    \n",
    "    \n",
    "    if new_val>baseline:\n",
    "        isBetter = True\n",
    "    print(\"isBetter: \"+str(isBetter))\n",
    "    \n",
    "    return isBetter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429e59f-798c-49a6-82e2-7c7854f34c8d",
   "metadata": {},
   "source": [
    "## 4. Predict Endpoint component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "70ea05e3-9efc-4fe1-9b31-14321326e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/googleapis/python-aiplatform/blob/master/samples/snippets/predict_custom_trained_model_sample.py\n",
    "#https://cloud.google.com/ai-platform/prediction/docs/online-predict\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def predict_endpoint_test(endpoint_id: Input[Artifact],\n",
    "                          location: str,\n",
    "                          project: str,\n",
    "                          api_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\n",
    "    \n",
    "    from typing import Dict\n",
    "    from google.cloud import aiplatform\n",
    "    from google.protobuf import json_format\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "    \n",
    "    print(endpoint_id)\n",
    "    endpoint_id = endpoint_id.uri.split('/')[-1]\n",
    "    print(endpoint_id)\n",
    "    \n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    #https://machinelearningmastery.com/make-predictions-scikit-learn/\n",
    "    instance_dict = [ 1.74481176,  0.86540763, -1.07296862 ,-2.3015387,  -2.06014071, 1.46210794, 0.3190391 , -0.24937038 ,-0.61175641 ,-0.7612069 , -0.38405435, -0.52817175, -0.3224172,   1.62434536]\n",
    "    \n",
    "    instance = json_format.ParseDict(instance_dict, Value())\n",
    "    instances = [instance]\n",
    "    \n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    \n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        print(\" prediction:\" + str(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "847e244b-6007-4e34-8887-877cea41c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "81684b00-a7df-428d-a9e1-92faf21dd6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"train-scikit\" + str(uuid.uuid4()))\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    bucket: str = BUCKET_NAME,\n",
    "    baseline_accuracy: float = 70.0\n",
    "):\n",
    "    preprocess_task = preprocess()\n",
    "    \n",
    "    train_task = train(preprocess_task.output, model_type='linear_regression', project_id=PROJECT_ID, bucket=BUCKET_NAME)\n",
    "    train_task2 = train(preprocess_task.output, model_type='naive_bayes', project_id=PROJECT_ID, bucket=BUCKET_NAME)\n",
    "    train_task3 = train(preprocess_task.output, model_type='decision_tree', project_id=PROJECT_ID, bucket=BUCKET_NAME)\n",
    "    \n",
    "    eval_task = eval(baseline_accuracy, train_task.outputs[\"accuracy\"], train_task2.outputs[\"accuracy\"], train_task3.outputs[\"accuracy\"])\n",
    "    \n",
    "    with dsl.Condition(eval_task.output == \"true\", name=\"eval models\"):\n",
    "        model_upload_op = gcc_aip.ModelUploadOp(\n",
    "            project=PROJECT_ID,\n",
    "            display_name=\"model\"+TIMESTAMP, \n",
    "    #        artifact_uri=\"gs://user-group-demo/pipeline_root/141610882258/train-scikitf989f632-b955-4bb1-a72d-0480d1c08627-20210620145355/train_-6780204423378370560/\", # GCS location of model\n",
    "            artifact_uri=train_task.outputs[\"artifact_uri\"], # GCS location of model\n",
    "            serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\",\n",
    "        )\n",
    "\n",
    "        endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "            project=PROJECT_ID,\n",
    "            display_name=\"pipelines\"+TIMESTAMP,\n",
    "        )\n",
    "\n",
    "        model_deploy_op = gcc_aip.ModelDeployOp( \n",
    "            project=PROJECT_ID,\n",
    "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            deployed_model_display_name=\"model_display_name\",\n",
    "            machine_type=\"n1-standard-4\",\n",
    "        )\n",
    "        \n",
    "        predict_task = predict_endpoint_test(project=PROJECT_ID, location=REGION, endpoint_id = model_deploy_op.outputs['endpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e3b37e84-3ed5-4c69-87d1-cf19d27a4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, \n",
    "                            package_path=\"dag-\"+TIMESTAMP+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "37927c38-8ab2-45be-8d27-156de5123c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4523294d-c363-479a-9198-4995431f67fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/train-scikitf78d4738-174e-49a8-a38d-ab97dd2a47fb-20210713233022?project=kubeflow-demos\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    \"dag-\"+TIMESTAMP+\".json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878ae2a-cbcb-400a-b714-f2d32a70a3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
